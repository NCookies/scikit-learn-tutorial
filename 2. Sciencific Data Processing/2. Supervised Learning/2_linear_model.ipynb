{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "diabetes_X_train = diabetes.data[:-20]\n",
    "diabetes_X_test  = diabetes.data[-20:]\n",
    "diabetes_y_train = diabetes.target[:-20]\n",
    "diabetes_y_test  = diabetes.target[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# fit parameters of hypothesis \n",
    "regr.fit(diabetes_X_train, diabetes_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.03499549e-01,  -2.37639315e+02,   5.10530605e+02,\n         3.27736980e+02,  -8.14131709e+02,   4.92814588e+02,\n         1.02848452e+02,   1.84606489e+02,   7.43519617e+02,\n         7.60951722e+01])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copy_X', 'fit_intercept', 'n_jobs', 'normalize']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr._get_param_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2004.5676026898207"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean((regr.predict(diabetes_X_test)-diabetes_y_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 197.61846908  155.43979328  172.88665147  111.53537279  164.80054784\n  131.06954875  259.12237761  100.47935157  117.0601052   124.30503555\n  218.36632793   61.19831284  132.25046751  120.3332925    52.54458691\n  194.03798088  102.57139702  123.56604987  211.0346317    52.60335674]\n[ 233.   91.  111.  152.  120.   67.  310.   94.  183.   66.  173.   72.\n   49.   64.   48.  178.  104.  132.  220.   57.]\n"
     ]
    }
   ],
   "source": [
    "print(regr.predict(diabetes_X_test))\n",
    "print(diabetes_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58507530226905746"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.score(diabetes_X_test, diabetes_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "위의 결과들을 보면 알겠지만 정확도가 별로 높지 않음\n",
       "</br>\n",
       "score 메서드는 Returns the coefficient of determination R^2 of the prediction. 라고 설명되어 있는데 np.mean(~~)한 식과 같은 내용이다.\n",
       "</br>\n",
       "각 차원마다 data point(?)가 적으면 예측한 값들의 variance 가 높다(= 데이터들이 분산되어 있음)"
      ],
      "text/plain": [
       "위의 결과들을 보면 알겠지만 정확도가 별로 높지 않음\n",
       "</br>\n",
       "score 메서드는 Returns the coefficient of determination R^2 of the prediction. 라고 설명되어 있는데 np.mean(~~)한 식과 같은 내용이다.\n",
       "</br>\n",
       "각 차원마다 data point(?)가 적으면 예측한 값들의 variance 가 높다(= 데이터들이 분산되어 있음)"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "위의 결과들을 보면 알겠지만 정확도가 별로 높지 않음\n",
    "</br>\n",
    "score 메서드는 Returns the coefficient of determination R^2 of the prediction. 라고 설명되어 있는데 np.mean(~~)한 식과 같은 내용이다.\n",
    "</br>\n",
    "각 차원마다 data point(?)가 적으면 예측한 값들의 variance 가 높다(= 데이터들이 분산되어 있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5]\n [ 1. ]] [0.5, 1] [[0]\n [2]]\n"
     ]
    }
   ],
   "source": [
    "X = np.c_[.5, 1].T\n",
    "y = [.5, 1]\n",
    "test = np.c_[0, 2].T\n",
    "print(X, y, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id='5bf9d9e1-18d7-4cc6-9b60-5156eb98fe79'></div>"
      ],
      "text/plain": [
       "<div id='5bf9d9e1-18d7-4cc6-9b60-5156eb98fe79'></div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_X :  [[ 0.67640523]\n [ 1.04001572]]\n[[ 0.5]\n [ 1. ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this_X :  [[ 0.5978738 ]\n [ 1.22408932]]\n[[ 0.5]\n [ 1. ]]\nthis_X :  [[ 0.6867558 ]\n [ 0.90227221]]\n[[ 0.5]\n [ 1. ]]\nthis_X :  [[ 0.59500884]\n [ 0.98486428]]\n[[ 0.5]\n [ 1. ]]\nthis_X :  [[ 0.48967811]\n [ 1.04105985]]\n[[ 0.5]\n [ 1. ]]\nthis_X :  [[ 0.51440436]\n [ 1.14542735]]\n[[ 0.5]\n [ 1. ]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "\n",
    "np.random.seed(0)\n",
    "for _ in range(6):\n",
    "    this_X = .1 * np.random.normal(size=(2, 1)) + X  # high variance\n",
    "    print('this_X : ', this_X)\n",
    "    print(X)\n",
    "    regr.fit(X, y)\n",
    "    plt.plot(test, regr.predict(test))\n",
    "    plt.scatter(this_X, y, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div id='04708952-9dc6-46a0-8b5c-68688dabca0a'></div>"
      ],
      "text/plain": [
       "<div id='04708952-9dc6-46a0-8b5c-68688dabca0a'></div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "regr = linear_model.Ridge(alpha=.1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "np.random.seed(0)\n",
    "for _ in range(6):\n",
    "    this_X = .1*np.random.normal(size=(2, 1)) + X\n",
    "    regr.fit(this_X, y)\n",
    "    plt.plot(test, regr.predict(test))\n",
    "    plt.scatter(this_X, y, s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "확률적인 방법으로 high-dimensional 에서 높은 variance를 낮추는 해결책으로 Ridge 가 있음.\n",
       "bias/variance tradeoff 예시임.\n",
       "Ridge의 alpha 값이 높을수록 bias 는 높아지고 variance 는 낮아짐 => underfitting"
      ],
      "text/plain": [
       "확률적인 방법으로 high-dimensional 에서 높은 variance를 낮추는 해결책으로 Ridge 가 있음.\n",
       "bias/variance tradeoff 예시임.\n",
       "Ridge의 alpha 값이 높을수록 bias 는 높아지고 variance 는 낮아짐 => underfitting"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "확률적인 방법으로 high-dimensional 에서 높은 variance를 낮추는 해결책으로 Ridge 가 있음.\n",
    "bias/variance tradeoff 예시임.\n",
    "Ridge의 alpha 값이 높을수록 bias 는 높아지고 variance 는 낮아짐 => underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58511106838835314, 0.58520730154446743, 0.58546775406984897, 0.58555120365039148, 0.58307170855541612, 0.570589994372801]\n"
     ]
    }
   ],
   "source": [
    "# We can choose alpha to minimize left out error, this time using the diabetes dataset rather than our synthetic data:\n",
    "alphas = np.logspace(-4, -1, 6)\n",
    "from __future__ import print_function\n",
    "print([regr.set_params(alpha=alpha\n",
    "                       ).fit(diabetes_X_train, diabetes_y_train,\n",
    "                             ).score(diabetes_X_test, diabetes_y_test) for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "차원을 줄이는 용도로 Ridge 외에 Lasso 라는 것이 있음"
      ],
      "text/plain": [
       "차원을 줄이는 용도로 Ridge 외에 Lasso 라는 것이 있음"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%html\n",
    "차원을 줄이는 용도로 Ridge 외에 Lasso 라는 것이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -212.43764548  517.19478111  313.77959962 -160.8303982    -0.\n -187.19554705   69.38229038  508.66011217   71.84239008]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "regr = linear_model.Lasso()\n",
    "alphas = np.logspace(-4, -1, 6)\n",
    "\n",
    "scores = [regr.set_params(alpha=alpha\n",
    "            ).fit(diabetes_X_train, diabetes_y_train\n",
    "            ).score(diabetes_X_test, diabetes_y_test)\n",
    "       for alpha in alphas]\n",
    "best_alpha = alphas[scores.index(max(scores))]\n",
    "regr.alpha = best_alpha\n",
    "regr.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "print(regr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris_X = iris.data\n",
    "iris_y = iris.target\n",
    "np.unique(iris_y)\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test  = iris_X[indices[-10:]]\n",
    "iris_y_test  = iris_y[indices[-10:]]\n",
    "\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "logistic.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1 0 0 0 2 1 2 0]\n[1 1 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "print(logistic.predict(iris_X_test))\n",
    "print(iris_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn Score : 0.980000\nLogisticRegression Score : 0.980000\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "X_digits_train = X_digits[:-50]\n",
    "y_digits_train = y_digits[:-50]\n",
    "\n",
    "X_digits_test = X_digits[-50:]\n",
    "y_digits_test = y_digits[-50:]\n",
    "\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_digits_train, y_digits_train)\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "logistic.fit(X_digits_train, y_digits_train)\n",
    "\n",
    "print('Knn Score : %f' % knn.score(X_digits_test, y_digits_test))\n",
    "print('LogisticRegression Score : %f' % logistic.score(X_digits_test, y_digits_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 0.961111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression score: 0.938889\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets, neighbors, linear_model\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "n_samples = len(X_digits)\n",
    "\n",
    "X_train = X_digits[:int(.9 * n_samples)]\n",
    "y_train = y_digits[:int(.9 * n_samples)]\n",
    "X_test = X_digits[int(.9 * n_samples):]\n",
    "y_test = y_digits[int(.9 * n_samples):]\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test))\n",
    "print('LogisticRegression score: %f'\n",
    "      % logistic.fit(X_train, y_train).score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n  decision_function_shape=None, degree=3, gamma='auto', kernel='poly',\n  max_iter=-1, probability=False, random_state=None, shrinking=True,\n  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='poly', degree=3)\n",
    "svc.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='rbf')\n",
    "# gamma: inverse of size of\n",
    "# radial kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}